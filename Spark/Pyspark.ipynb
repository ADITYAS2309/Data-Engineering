{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6103443a",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5018c59-d54c-4692-a4c6-dff008031b63",
   "metadata": {},
   "source": [
    "Spark is open source distributed computing engine designed to process large scale data quickly in parallel. It does this by keeping data in memorary rather than writing to disk after every operation. While there are multiple uses, we will be focusing on PySpark/Spark SQL\n",
    "\n",
    "### **Architecture:**\n",
    "\n",
    "Application -> Framework (Driver + Cluster Manager) -> Workers/Executors\n",
    "\n",
    "**Driver:**\n",
    "\n",
    "- Main Node (Brain)\n",
    "- Starts the Spark Application\n",
    "- Defines actions/transformations done on data\n",
    "- Coordintes with cluster manager\n",
    "- Collects final results from executors\n",
    "\n",
    "**Cluster Manager:**\n",
    "\n",
    "- Allocates resources to driver and executor nodes\n",
    "- Monitors health of worker/executor nodes\n",
    "- Cleans up resources after session\n",
    "- Egs: local, YARN, Standalone, Kubernetes\n",
    "- We need to submit via .master() method to connect with the cluster manager\n",
    "\n",
    "**Worker/Executor:**\n",
    "\n",
    "- Node where actual transformation happens\n",
    "- Each worker node contains slots, as manay slots as number of CPU cores\n",
    "- Data is partitioned and each partition is processed on a slot at a time; this is called Task, smallest unit of work\n",
    "\n",
    "\n",
    "Data Transformations are lazy, nothing is performed until asked for. This is done via actions like show/count/collect etc.\n",
    "\n",
    "Each job can be divided into stages and each stage into tasks. Stages are group of tasks with no data shuffling required.\n",
    "\n",
    "Data Shuffling:\n",
    "\n",
    "- Redistributing/exchanging data across partitions\n",
    "- Wide transformations (groupby, join) requires data shuffling\n",
    "- Requires saving data to disk, sending over network and reading again from disk\n",
    "\n",
    "Spark Jobs can be run on cloud platforms like:\n",
    "\n",
    "- Amazon EMR\n",
    "- Databricks\n",
    "- Synapse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a13e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc284b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")     # use all CPU cores\n",
    "    .appName(\"Basics\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # driver memory \n",
    "    .config(\"spark.executor.memory\", \"4g\")  # memory per executor \n",
    "    .config(\"spark.executor.cores\", \"4\")   # use multiple cores per executor \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16a7d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DYD:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cf61003530>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117be1aa-87ba-460f-8a96-d2b79ec2327e",
   "metadata": {},
   "source": [
    "We can visit the Spark UI through the above link to monitor job, their schedules, actions plan, spark session information.\n",
    "\n",
    "Submitting a spark job to a remote cluster can we done via CLI/bash and adding the .py file using SSH. Alternatively we can upload the script to say S3 and execute their. We can open jupyter notebooks on the above mentioned cloud platforms and execute directly. If there is an on-premise cloud service, we will need to provide the connection details via master/config commands or submit via SSH.\n",
    "\n",
    "**Majority of syntax is similar to pandas. There are almost all equivalent pandas command and which can be referred via the [documentation]().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c47f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv\n",
    "df = spark.read.csv([r\"C:\\Users\\dydmu\\Downloads\\dd\\yellow_tripdata_2016-03.csv\",\n",
    "                            r\"C:\\Users\\dydmu\\Downloads\\dd\\yellow_tripdata_2016-02.csv\",\n",
    "                            r\"C:\\Users\\dydmu\\Downloads\\dd\\yellow_tripdata_2016-01.csv\",\n",
    "                            r\"C:\\Users\\dydmu\\Downloads\\dd\\yellow_tripdata_2015-01.csv\"\n",
    "                          ]\n",
    "                          , header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f99e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47248845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() #Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaa003e-40f8-443f-889b-05e7b50be2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "417f26c0-da5d-4b98-9d04-f91d531c122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "#Equivalent to df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c2d22d2-d619-4206-b7cf-ddd4d76f74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|   pickup_latitude|RatecodeID|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       1| 2016-03-01 00:00:00|  2016-03-01 00:07:55|              1|         2.50|-73.97674560546875|40.765151977539062|         1|                 N|-74.004264831542969|40.746128082275391|           1|          9|  0.5|    0.5|      2.05|           0|                  0.3|       12.35|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5210fde1-15b0-47bc-8b2b-fccc18c23705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|store_and_fwd_flag|   count|\n",
      "+------------------+--------+\n",
      "|                 Y|  310683|\n",
      "|                 N|46938162|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('store_and_fwd_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b83695-da8b-44e3-a722-0c69a884d079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List out columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7994648a-275f-4bbc-b543-49b2203c67af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21759996"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering data\n",
    "df.filter(df.total_amount <= 10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47a20900-3fa4-4668-8df5-debd57c612db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column using withColumn\n",
    "#np.where equivalent is .when()\n",
    "df = df.withColumn('payment_type', F.when(df.payment_type == 1, 'cash').otherwise('credit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b1e8244-6c67-47ee-9761-ec112c510130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10052335"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((df.total_amount <= 10) & (df.payment_type == 'credit')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0657f43c-7ff9-48d0-a23e-5643a8c33b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------+\n",
      "|payment_type|passenger_count|   count|\n",
      "+------------+---------------+--------+\n",
      "|        cash|              4|  518657|\n",
      "|        cash|              1|22026778|\n",
      "|      credit|              5|  873114|\n",
      "|        cash|              5| 1678344|\n",
      "|      credit|              1|11259639|\n",
      "|      credit|              2| 2440027|\n",
      "|        cash|              3| 1172957|\n",
      "|      credit|              3|  729242|\n",
      "|        cash|              2| 4236662|\n",
      "|      credit|              0|    2995|\n",
      "|      credit|              4|  387071|\n",
      "|        cash|              0|    4768|\n",
      "+------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grouping the data\n",
    "#Can using multiple filter or use AND/OR\n",
    "(\n",
    "    df.filter(df.passenger_count <= 5)\n",
    "    .filter(df.store_and_fwd_flag == 'N') \n",
    "    .groupBy('payment_type', 'passenger_count')\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69d15ce9-261f-4ca8-9306-17f863724105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8214"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.passenger_count == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68426ba9-8457-44df-abe4-0da78bb029f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------+------------------+\n",
      "|payment_type|passenger_count|   count|               avg|\n",
      "+------------+---------------+--------+------------------+\n",
      "|        cash|              7|      58|45.523448275862066|\n",
      "|        cash|              4|  518657|  17.2535993922764|\n",
      "|        cash|              1|22026778| 16.99704312583126|\n",
      "|      credit|              5|  873114| 12.72630430848364|\n",
      "|        cash|              5| 1678344| 17.08351491708387|\n",
      "|      credit|              1|11259639|12.431929109800231|\n",
      "|        cash|              6| 1046978| 16.75319064965837|\n",
      "|      credit|              2| 2440027|13.538295752463975|\n",
      "|        cash|              3| 1172957| 17.09728693378995|\n",
      "|      credit|              3|  729242|13.579074669312753|\n",
      "|        cash|              2| 4236662|17.619134630054383|\n",
      "|      credit|              0|    2995|10.826020033388959|\n",
      "|      credit|              4|  387071|13.745014403043628|\n",
      "|      credit|              6|  560708|12.582556945862082|\n",
      "|        cash|              0|    4768|20.429679110738277|\n",
      "|        cash|              9|      49| 51.03183673469387|\n",
      "|        cash|              8|      63|             57.18|\n",
      "|      credit|              7|      21| 43.64666666666666|\n",
      "|      credit|              9|      17| 44.63176470588235|\n",
      "|      credit|              8|      14| 44.48571428571427|\n",
      "+------------+---------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multiple aggregations using .agg()\n",
    "(\n",
    "    df.filter(df.store_and_fwd_flag == 'N')\n",
    "    .groupBy('payment_type', 'passenger_count')\n",
    "    .agg(F.count('VendorID').alias('count'), F.avg('total_amount').alias('avg'))\n",
    "    .show()   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8295580-8478-4eaf-8324-0788439ec117",
   "metadata": {},
   "source": [
    "For querying in a SQL type manner, we create views. Views are temporary SQL table/views queried by Spark SQL. These can be restricted to individual/global sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7040656-c946-4452-8591-689d763411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create view\n",
    "df.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01932478-b767-4b52-a54e-531b1cf1abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------+------------------+\n",
      "|payment_type|passenger_count|   count|               avg|\n",
      "+------------+---------------+--------+------------------+\n",
      "|        cash|              7|      58|45.523448275862066|\n",
      "|        cash|              4|  518657|  17.2535993922764|\n",
      "|        cash|              1|22026778| 16.99704312583126|\n",
      "|      credit|              5|  873114| 12.72630430848364|\n",
      "|        cash|              5| 1678344| 17.08351491708387|\n",
      "|      credit|              1|11259639|12.431929109800231|\n",
      "|        cash|              6| 1046978| 16.75319064965837|\n",
      "|      credit|              2| 2440027|13.538295752463975|\n",
      "|        cash|              3| 1172957| 17.09728693378995|\n",
      "|      credit|              3|  729242|13.579074669312753|\n",
      "|        cash|              2| 4236662|17.619134630054383|\n",
      "|      credit|              0|    2995|10.826020033388959|\n",
      "|      credit|              4|  387071|13.745014403043628|\n",
      "|      credit|              6|  560708|12.582556945862082|\n",
      "|        cash|              0|    4768|20.429679110738277|\n",
      "|        cash|              9|      49| 51.03183673469387|\n",
      "|        cash|              8|      63|             57.18|\n",
      "|      credit|              7|      21| 43.64666666666666|\n",
      "|      credit|              9|      17| 44.63176470588235|\n",
      "|      credit|              8|      14| 44.48571428571427|\n",
      "+------------+---------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select payment_type, passenger_count, count(*) as count, avg(total_amount) as avg\n",
    "          from data\n",
    "          where store_and_fwd_flag = \"N\"\n",
    "          group by payment_type, passenger_count\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5c2ad-9940-406a-8b7f-60cdcd075aee",
   "metadata": {},
   "source": [
    "### Connecting and querying database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecab59-9926-422d-b1c4-97c277317a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/my_database\"\n",
    "table_name = \"my_table\"\n",
    "properties = {\n",
    "    \"user\": \"my_user\",\n",
    "    \"password\": \"my_password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"   # MySQL 8+ driver\n",
    "}\n",
    "\n",
    "query = \"\"\"\n",
    "(select *\n",
    "from my_table\n",
    "where column <= N) as my_query\n",
    "\"\"\"\n",
    "\n",
    "df = spark.read.jdbc(url=jdbc_url, table=query, properties=properties,\n",
    "                    column=\"id\",      # numeric column for partitioning\n",
    "                    lowerBound=1,     # min value of column\n",
    "                    upperBound=10000, # max value of column\n",
    "                    numPartitions=10)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3aa770-09f0-47a1-b65b-7a470e0762b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data\n",
    "\n",
    "df.write.jdbc(url=jdbc_url, \n",
    "              table=\"my_new_table\", \n",
    "              mode=\"append\",\n",
    "              properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c699ec0-ccc9-4ac1-b3d8-95e936951112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52d1e1-e857-46cc-b07e-6c8ea180e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773502ca-5592-4e2c-bd29-ffb9cc450866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
